#!/usr/bin/env bash
set -e



# start sshd server
_sshd_host() {
  if [ ! -d /var/run/sshd ]; then
    mkdir /var/run/sshd
    ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N ''
  fi
  /usr/sbin/sshd
}

# setup worker ssh to be passwordless
_ssh_worker() {
  if [[ ! -d /home/worker ]]; then
    mkdir -p /home/worker
    chown -R worker:worker /home/worker
  fi
  cat > /home/worker/setup-worker-ssh.sh <<'EOF2'
mkdir -p ~/.ssh
chmod 0700 ~/.ssh
ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N "" -C "$(whoami)@$(hostname)-$(date -I)"
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
chmod 0640 ~/.ssh/authorized_keys
cat >> ~/.ssh/config <<EOF
Host *
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
  LogLevel QUIET
EOF
chmod 0644 ~/.ssh/config
cd ~/
tar -czvf ~/worker-secret.tar.gz .ssh
cd -
EOF2
  chmod +x /home/worker/setup-worker-ssh.sh
  chown worker: /home/worker/setup-worker-ssh.sh
  sudo -u worker /home/worker/setup-worker-ssh.sh
}

# start munge and generate key
_munge_start() {
  # Create necessary directories if they do not exist
  mkdir -p /var/run/munge /etc/munge /var/log/munge /var/lib/munge
# Restrict the permissions of the Munge directories
  chown -R munge: /etc/munge/
 chown -R munge: /var/log/munge/
 chown -R munge: /var/lib/munge/
 chown -R munge: /run/munge/
 chmod 0700 /etc/munge/
 chmod 0700 /var/log/munge/
 chmod 0700 /var/lib/munge/
 chmod 0700 /run/munge/
 chmod a+x /run/munge
 chown munge:munge /etc/munge/munge.key
sudo chmod 400 /etc/munge/munge.key

 # Start the Munge daemon
  sudo -u munge /usr/sbin/munged 


  # Test Munge operation
  munge -n
  munge -n | unmunge


 # Test Munge operation
  munge -n
  munge -n | unmunge


 service munge start 
}

# copy secrets to /.secret directory for other nodes
_copy_secrets() {
  cp /home/worker/worker-secret.tar.gz /.secret/worker-secret.tar.gz
  cp /home/worker/setup-worker-ssh.sh /.secret/setup-worker-ssh.sh
   cp /etc/munge/munge.key /.secret/munge.key
  rm -f /home/worker/worker-secret.tar.gz
  rm -f /home/worker/setup-worker-ssh.sh
}


# Wait for database to be ready
_wait_for_database() {
  echo -n "Waiting for database to be ready"
  until mysql -h database.local.dev -u $STORAGE_USER -p$STORAGE_PASS -e 'SELECT 1' > /dev/null 2>&1; do
    echo -n "."
    sleep 1
  done
  echo ""
}



# generate slurm.conf
_generate_slurm_conf() {
  cat > /etc/slurm/slurm.conf <<EOF
#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=$CLUSTER_NAME
SlurmctldHost=$CONTROL_MACHINE
#SlurmctldHostr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=$SLURMCTLD_PORT
SlurmdPort=$SLURMD_PORT
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=database.local.dev 
AccountingStoragePort=3306
#AccountingStorageLoc=slurm_acct_db
AccountingStorageUser=$STORAGE_USER
AccountingStoragePass=$STORAGE_PASS

# COMPUTE NODES
NodeName=worker[01-02] RealMemory=1800 CPUs=1 State=UNKNOWN
PartitionName=$PARTITION_NAME Nodes=ALL Default=YES MaxTime=INFINITE State=UP
EOF
}


# run slurmctld
_slurmctld() {
  if $USE_SLURMDBD; then
    echo -n "cheking for slurmdbd.conf"
    while [ ! -f /.secret/slurmdbd.conf ]; do
      echo -n "."
      sleep 1
    done
    echo ""
  fi
  mkdir -p /var/spool/slurm/ctld \
    /var/spool/slurm/d \
    /var/log/slurm
  chown -R slurm: /var/spool/slurm/ctld \
    /var/spool/slurm/d \
    /var/log/slurm
  touch /var/log/slurmctld.log
  chown slurm: /var/log/slurmctld.log
  if [[ ! -f /home/config/slurm.conf ]]; then
    echo "### generate slurm.conf ###"
    _generate_slurm_conf
  else
    echo "### use provided slurm.conf ###"
    cp /home/config/slurm.conf /etc/slurm/slurm.conf 
  fi
  sacctmgr -i add cluster "${CLUSTER_NAME}"
  sleep 2s
    service slurmctld start
  cp -f /etc/slurm/slurm.conf /.secret/
  slurmctld -D

}

# Function to check if slurmctld is running
_check_slurmctld() {
  for i in {1..10}; do # 10 attempts to see if slurmctld is up
    if service slurmctld status | grep -q "is running"; then
      echo "slurmctld is up and running!"
      return 0
    fi
    echo "Waiting for slurmctld to be up..."
    sleep 5
  done
  echo "slurmctld is not running! Exiting..."
  exit 1
}

# Function to print logs
_print_logs() {
  echo "### Printing logs for slurm ###"
  cat /var/log/slurm/slurm.log
  echo "### Printing logs for slurmctld ###"
  cat /var/log/slurm/slurmctld.log
  echo "### Printing logs for munge ###"
  cat /var/log/munge/munged.log
  echo "### Printing logs for slurmdbd ###"
  cat /var/log/slurm/slurmdbd.log
}

### main ###
_sshd_host
_ssh_worker
_munge_start
_copy_secrets
_wait_for_database
_slurmctld
_check_slurmctld
_print_logs

tail -f /dev/null
cat /etc/slurm/slurm.conf